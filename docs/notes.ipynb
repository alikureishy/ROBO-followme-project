{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flips all the images horizontally to double the data\n",
    "# (From: https://udacity-robotics.slack.com/archives/C7A5HT92M/p1507226777000533)\n",
    "import os\n",
    "import glob\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "def flip_and_save_images(img_dir, extension):\n",
    "  os.chdir(img_dir)\n",
    "  files = glob.glob(\"*.\" + extension)\n",
    "  for i, file in enumerate(files):\n",
    "    print(i)\n",
    "    img = misc.imread(file, flatten=False, mode='RGB')\n",
    "    flipped_img = np.fliplr(img)\n",
    "    misc.imsave(\"flipped\" + file, flipped_img)\n",
    "\n",
    "################\n",
    "flip_and_save_images(train_mask_directory, \"png\")\n",
    "flip_and_save_images(train_images_directory, \"jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding checkpoints for each epoch, to see how each one did relative to the previous one\n",
    "# (From: https://udacity-robotics.slack.com/archives/C7A5HT92M/p1507151193000223)\n",
    "tensorBoard_cb = keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.h5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, period=1)\n",
    "callbacks = [tensorBoard_cb, checkpoint_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to delete raw images that do NOT contain the hero:\n",
    "# (From: https://udacity-robotics.slack.com/archives/C7A5HT92M/p1507092634000032)\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "def get_cam3_files(files):\n",
    "    is_cam3 = lambda x: x.find('cam3_') != -1\n",
    "    return sorted(list(filter(is_cam3, files)))\n",
    "\n",
    "\n",
    "def get_cam3_file_id(filename, base_path):\n",
    "    return filename.replace(base_path,'').replace('/','').replace('cam3_','').replace('.png','')\n",
    "\n",
    "\n",
    "def delete_all_cam_files(id, path):\n",
    "    c1 = path+'/'+'cam1_'+id+'.png'\n",
    "    c2 = path+'/'+'cam2_'+id+'.png'\n",
    "    c3 = path+'/'+'cam3_'+id+'.png'\n",
    "    c4 = path+'/'+'cam4_'+id+'.png'\n",
    "    delete_file(c1)\n",
    "    delete_file(c2)\n",
    "    delete_file(c3)\n",
    "    delete_file(c4)\n",
    "\n",
    "\n",
    "def delete_file(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def contains_hero(filename):\n",
    "    # Commenting next 2 lines based on post: https://udacity-robotics.slack.com/archives/C7A5HT92M/p1507092955000027?thread_ts=1507092634.000032&cid=C7A5HT92M\n",
    "    #    s = np.sum(misc.imread(filename)[:,:,0])\n",
    "    #    return s < 16711680\n",
    "    # Instead, we use this:\n",
    "    return misc.imread(filename)[:,:,0].max() == 255 # (From: https://udacity-robotics.slack.com/archives/C7A5HT92M/p1507092955000027?thread_ts=1507092634.000032&cid=C7A5HT92M)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('path',\n",
    "                        help='The image path to filter')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_path = args.path\n",
    "    files = glob.glob(os.path.join(base_path, '*.png'))\n",
    "    cam3 = get_cam3_files(files)\n",
    "    for f in cam3:\n",
    "        if(not contains_hero(f)):\n",
    "            id = get_cam3_file_id(f, base_path)\n",
    "            delete_all_cam_files(id, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To determine % of files that contain teh hero:\n",
    "# (From: https://udacity-robotics.slack.com/files/U4UKR0C5Q/F7DTF3D1C/Script_to_see_what___of_training_image_masks_contain_the_hero.py)\n",
    "#By tokyo_adam 4-10-17 \n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "#set to the directory where your masks are saved\n",
    "img_dir = \"../data/train/masks/\"\n",
    "\n",
    "total_files = 0\n",
    "total_hero = 0\n",
    "\n",
    "os.chdir(img_dir)\n",
    "for file in glob.glob(\"*.png\"):\n",
    "\ttotal_files +=1\n",
    "\n",
    "\timg = cv2.imread(file)\n",
    "\tblue = img[:,:,0]\n",
    "\n",
    "\tif np.any(blue == 255):\n",
    "\t\ttotal_hero += 1\n",
    "\n",
    "percent_hero = 100. * total_hero / total_files\n",
    "\n",
    "print (percent_hero, \"percent of files contain the hero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator Instructions\n",
    "\n",
    "During data collection, when does the hero and crowd spawn? Here are the steps i follow:\n",
    "* mark points for quad patrol as well as hero movement using `P`, `O` and `I`. \n",
    "* Switch from local control to patrol/follow mode using `H`.\n",
    "* Press `M` to spawn the hero and crowd ? This step is actually not very clear from the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last point was:\n",
    "https://udacity-robotics.slack.com/archives/C7A5HT92M/p1507278812000121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3dbc358a12ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#logger_cb = plotting_tools.LoggerPlotter()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogger_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./logs/model1c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mwrite_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0membeddings_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msave_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./saved_chkpts/model1c/weights-e{epoch:02d}-{val_loss:.2f}.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# Logging callback for Tensorboard\n",
    "# (From: https://udacity-robotics.slack.com/archives/C7A5HT92M/p1507762674000185?thread_ts=1507760279.000071&cid=C7A5HT92M)\n",
    "# \n",
    "#logger_cb = plotting_tools.LoggerPlotter()\n",
    "logger_cb = keras.callbacks.TensorBoard(log_dir='./logs/model1c', histogram_freq=1, batch_size=32, \\\n",
    "  write_graph=True, write_grads=False, write_images=False, \\\n",
    "  embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "save_cb = keras.callbacks.ModelCheckpoint(filepath='./saved_chkpts/model1c/weights-e{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, period=5)\n",
    "\n",
    "callbacks = [logger_cb, save_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the base dataset, after every run i see that my `iou3` (when hero is far) is very low which is bringing down the final score. I think this is because of less images where hero is far away in the total dataset. I am working on adding more images to the dataset to balance out the classes.\n",
    "\n",
    "For me the encoder block did matter. Having 2 convolution layers that do not change the size of the layer, before the third convolution that also resizes the layer by using a stride of 2.\n",
    "\n",
    "I also used 4-5 encoding blocks and the same number of decoding (although not sure that it actually mattered)\n",
    "\n",
    "Now 33% is very low and I cannot explain it.\n",
    "\n",
    "Use a lot of epochs.. 100 for example... and do not use all the data for each epoch. The problem is that the dataset is not balanced , so you will probably not get a good result (there are too few images with the target far away)\n",
    "\n",
    "Here are the counts from training data\n",
    "```no hero = 2576\n",
    "hero very far = 521\n",
    "hero not too close = 621\n",
    "hero close = 413```\n",
    "\n",
    "These are calculated based on the number of blue pixels in the `mask.png`. the size of the mask is `256x256`\n",
    "```no hero = 0 pixels\n",
    "hero very far = 1 to 40 pixels\n",
    "hero not tool close = 40 to 400 pixels\n",
    "hero close = > 400 pixels```\n",
    "\n",
    "I was able to get 41.6% with base data and without changing the kernel size, i used\n",
    "```lr = 0.001\n",
    "epoch = 12\n",
    "batch size = 32\n",
    "```\n",
    "I used 4 encoder layers and 4 decoder layers. strides 2 and upsampled by 2. I tried a lot of different variations but 43% is the max i could do. 20 is my max epochs and i seem to saturate before that.\n",
    "\n",
    "At every step, the code randomly picks `batch_size` number of images to train. so, steps per epoch is basically how many times to you do that in every epoch. training loss is getting updated at the end of every step and validation loss is getting updated at the end of every epoch.\n",
    "\n",
    "With just the provided data I could achieve ~41%. Could improve a bit more with adding my own data of the hero. I would echo @annnn's suggestion to try some experiments with model depth and hyperparams! Btw in my experience making really deep model actually made the final performance worse (I presume overfitting) so your milage may vary just with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model:\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, period=1)\n",
    "\n",
    "#...and add it to the callbacks list where the plotting function is called. You can tune what is saved in the checkpoint. That way you can select which ever epoch performs best\n",
    "\n",
    "# Eg:\n",
    "logger_cb = plotting_tools.LoggerPlotter()\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, period=1)\n",
    "\n",
    "callbacks = [logger_cb,checkpoint_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best thing to increase the score: collecting your own data. Focus on hero far away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout method is usually used in the last few layers. However, it would lead to slow convergence speed and long training time. As mentioned by @chedanix, if you don't train your model with hundreds  of epochs or you don't notice there is overfitting issue for your model, there is no need to use dropout from my experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can add `model.summary()` before `model.fit_generator` to see the entire summary of the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing throgh keras\n",
    "https://keras.io/visualization/\n",
    "\n",
    "You can use Keras to generate a figure of your network:\n",
    "If you run a cell with `plotting_tools.plot_keras_model(model, 'name_of_fig')` there will be two plots generated in `/data/figures`. I've used the one with _shapes at the end.\n",
    "\n",
    "Also, run \"conda update --all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip\n",
    "Try to have more conv layers in encoders and decoders. And turn on the image augmentation. Then set steps of each epoch around 20 to 50.\n",
    "\n",
    "https://udacity-robotics.slack.com/archives/C7A5HT92M/p1511798020000230\n",
    "\n",
    "https://udacity-robotics.slack.com/archives/C7A5HT92M/p1513618303000105\n",
    "    Here's the network image: https://udacity-robotics.slack.com/files/U73U6DK55/F8GP5GQ6R/model.png\n",
    "    https://udacity-robotics.slack.com/files/U73U6DK55/F8GGSKT2P/image.png\n",
    "    https://udacity-robotics.slack.com/files/U73U6DK55/F8GP7RR9T/image.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to check if I'm using the GPU afterall?:\n",
    "> In a terminal, type nvidia-smi, on the same machine you are training on. That page will show you the gpu utilization.\n",
    "\n",
    "> Check this thread out as well if you are using AWS https://udacity-robotics.slack.com/archives/C7A5HT92M/p1510501438000047?thread_ts=1510501438.000047&cid=C7A5HT92M\n",
    "\n",
    "> https://udacity-robotics.slack.com/archives/C7A5HT92M/p1511251327000239\n",
    "  https://udacity-robotics.slack.com/archives/C7A5HT92M/p1511758359000169\n",
    "  https://udacity-robotics.slack.com/archives/C7A5HT92M/p1511917591000148\n",
    "  \n",
    "For persistent Jupyter session:\n",
    "https://udacity-robotics.slack.com/archives/C7A5HT92M/p1510929720000073\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About encoders and decoders and 1x1:\n",
    "https://udacity-robotics.slack.com/archives/C7A5HT92M/p1511695064000027\n",
    "https://udacity-robotics.slack.com/archives/C7A5HT92M/p1511895827000222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frmo: https://udacity-robotics.slack.com/files/U683C0H52/F8JCWV599/-.py\n",
    "class BatchIteratorSimple(Iterator):\n",
    "    def __init__(self, data_folder, batch_size, image_shape,\n",
    "            num_classes=3, training=True, shuffle=True, seed=None, shift_aug=False):\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.shift_aug = shift_aug\n",
    "        self.data_folder = data_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.training = training\n",
    "        self.image_shape = tuple(image_shape)\n",
    "\n",
    "        im_files = sorted(glob(os.path.join(data_folder, 'images', '*.jpeg')))\n",
    "        mask_files = sorted(glob(os.path.join(data_folder, 'masks', '*.png')))\n",
    "\n",
    "        if len(im_files) == 0:\n",
    "            raise ValueError('No image files found, check your image diractories')\n",
    "\n",
    "        if len(mask_files) == 0:\n",
    "            raise ValueError('No mask files found, check your mask directories')\n",
    "\n",
    "        self.file_tuples = list(zip(im_files, mask_files))\n",
    "        self.n = len(self.file_tuples)\n",
    "\n",
    "        super(BatchIteratorSimple, self).__init__(self.n, batch_size, shuffle, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to visualize the NN:\n",
    "from tensorflow.contrib.keras.python.keras.utils import plot_model\n",
    "import pydot\n",
    "\n",
    "#scroll down to the training section and beaeath the existing line...:\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss='categorical_crossentropy')\n",
    "#... add this line\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on local GPU:\n",
    "https://udacity-robotics.slack.com/archives/C7A5HT92M/p1514928010000521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You need to install, into your RoboND environment:\n",
    "* - Keras: * Use\n",
    "> pip install git+git://github.com/keras-team/keras.git\n",
    "Anything but this version will not work.\n",
    "\n",
    "* - Tensorflow GPU: * Use\n",
    "> pip install tensorflow-gpu==1.3\n",
    "*Do not remove Tensorflow 1.2.1*\n",
    "\n",
    "The correct versions of CUDNN and cudatoolkit should install automatically, in this case cuda 8 and CUDNN 6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
