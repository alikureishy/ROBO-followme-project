{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow-Me Project\n",
    "Congratulations on reaching the final project of the Robotics Nanodegree! \n",
    "\n",
    "Previously, you worked on the Semantic Segmentation lab where you built a deep learning network that locates a particular human target within an image. For this project, you will utilize what you implemented and learned from that lab and extend it to train a deep learning model that will allow a simulated quadcopter to follow around the person that it detects! \n",
    "\n",
    "Most of the code below is similar to the lab with some minor modifications. You can start with your existing solution, and modify and improve upon it to train the best possible model for this task.\n",
    "\n",
    "You can click on any of the following to quickly jump to that part of this notebook:\n",
    "1. [Data Collection](#data)\n",
    "2. [FCN Layers](#fcn)\n",
    "3. [Build the Model](#build)\n",
    "4. [Training](#training)\n",
    "5. [Prediction](#prediction)\n",
    "6. [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection<a id='data'></a>\n",
    "We have provided you with a starting dataset for this project. Download instructions can be found in the README for this project's repo.\n",
    "Alternatively, you can collect additional data of your own to improve your model. Check out the \"Collecting Data\" section in the Project Lesson in the Classroom for more details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safdar/anaconda3/envs/RoboND/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.keras.python import keras\n",
    "from tensorflow.contrib.keras.python.keras import layers, models\n",
    "#from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from tensorflow import image\n",
    "\n",
    "from utils import scoring_utils\n",
    "from utils.separable_conv2d import SeparableConv2DKeras, BilinearUpSampling2D\n",
    "from utils import data_iterator\n",
    "from utils import plotting_tools \n",
    "from utils import model_tools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir:  /Users/safdar/Documents/robond/RoboND-FollowMe/code\n",
      "Files found:  4131\n",
      "37.64221738077947 percent of files contain the hero\n"
     ]
    }
   ],
   "source": [
    "# To determine % of files that contain teh hero:\n",
    "# (From: https://udacity-robotics.slack.com/files/U4UKR0C5Q/F7DTF3D1C/Script_to_see_what___of_training_image_masks_contain_the_hero.py)\n",
    "#By tokyo_adam 4-10-17 \n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "#set to the directory where your masks are saved\n",
    "print (\"Current dir: \", os.getcwd())\n",
    "img_dir = \"../data/masks/train/set0\"\n",
    "\n",
    "total_files = 0\n",
    "total_hero = 0\n",
    "\n",
    "#os.chdir(img_dir)\n",
    "files = glob.glob(os.path.join(img_dir, 'masks', \"*.png\"))\n",
    "print (\"Files found: \", len(files))\n",
    "if (len(files) > 0):\n",
    "    for file in files:\n",
    "        total_files +=1\n",
    "\n",
    "        img = cv2.imread(file)\n",
    "        blue = img[:,:,0]\n",
    "\n",
    "        if np.any(blue == 255):\n",
    "            total_hero += 1\n",
    "    percent_hero = 100. * total_hero / total_files\n",
    "    print (percent_hero, \"percent of files contain the hero\")\n",
    "else:\n",
    "    print (\"No files found in folder: \", img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN Layers <a id='fcn'></a>\n",
    "In the Classroom, we discussed the different layers that constitute a fully convolutional network (FCN). The following code will introduce you to the functions that you need to build your semantic segmentation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separable Convolutions\n",
    "The Encoder for your FCN will essentially require separable convolution layers, due to their advantages as explained in the classroom. The 1x1 convolution layer in the FCN, however, is a regular convolution. Implementations for both are provided below for your use. Each includes batch normalization with the ReLU activation function applied to the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separable_conv2d_batchnorm(input_layer, filters, kernel_size, strides, name=\"sep-convolution-w-batchnorm\"):\n",
    "    with tf.name_scope(name):\n",
    "        output_layer = SeparableConv2DKeras(filters=filters,kernel_size=kernel_size, strides=strides,\n",
    "                                            padding='same', activation='relu'\n",
    "                                            #, name='separable-conv-layer'\n",
    "                                            )(input_layer)\n",
    "\n",
    "        output_layer = layers.BatchNormalization(\n",
    "                                                #name=\"batch-normalization-layer\"\n",
    "                                                )(output_layer)\n",
    "        return output_layer\n",
    "\n",
    "def conv2d_batchnorm(input_layer, filters, kernel_size, strides, name=\"convolution-w-batchnorm\"):\n",
    "    with tf.name_scope(name):\n",
    "        output_layer = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, \n",
    "                                     padding='same', activation='relu'\n",
    "                                     #, name='convolution-layer'\n",
    "                                    )(input_layer)\n",
    "\n",
    "        output_layer = layers.BatchNormalization(\n",
    "                                                #name=\"batch-normalization-layer\"\n",
    "                                                )(output_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear Upsampling\n",
    "The following helper function implements the bilinear upsampling layer. Upsampling by a factor of 2 is generally recommended, but you can try out different factors as well. Upsampling is used in the decoder block of the FCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_upsample(input_layer, multiplier, name=\"bi-upsample\"):\n",
    "    with tf.name_scope(name):\n",
    "        output_layer = BilinearUpSampling2D((multiplier, multiplier)\n",
    "                                            #, name=\"upsampler-layer\"\n",
    "                                           )(input_layer)\n",
    "#         tf.summary.histogram('weights', output_layer.get_weights()[0])\n",
    "#         tf.summary.histogram('biases', output_layer.get_weights()[1])\n",
    "#         tf.summary.histogram('biases', output_layer.output)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model <a id='build'></a>\n",
    "In the following cells, you will build an FCN to train a model to detect and locate the hero target within an image. The steps are:\n",
    "- Create an `encoder_block`\n",
    "- Create a `decoder_block`\n",
    "- Build the FCN consisting of encoder block(s), a 1x1 convolution, and decoder block(s).  This step requires experimentation with different numbers of layers and filter sizes to build your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "Create an encoder block that includes a separable convolution layer using the `separable_conv2d_batchnorm()` function. The `filters` parameter defines the size or depth of the output layer. For example, 32 or 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(input_layer, filters, kernel_size, strides, name=\"encoder-block\"):\n",
    "    with tf.name_scope(name):\n",
    "        # TODO Create a separable convolution layer using the separable_conv2d_batchnorm() function.\n",
    "        output_layer = separable_conv2d_batchnorm(input_layer, filters, kernel_size=kernel_size, strides=strides, name=\"separable-convolution-block\")\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "The decoder block is comprised of three parts:\n",
    "- A bilinear upsampling layer using the upsample_bilinear() function. The current recommended factor for upsampling is set to 2.\n",
    "- A layer concatenation step. This step is similar to skip connections. You will concatenate the upsampled small_ip_layer and the large_ip_layer.\n",
    "- Some (one or two) additional separable convolution layers to extract some more spatial information from prior layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(small_ip_layer, large_ip_layer, filters, multiplier, kernel_size, name=\"decoder-block\"):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        # TODO Upsample the small input layer using the bilinear_upsample() function.\n",
    "        upsampled_layer = bilinear_upsample(small_ip_layer, multiplier=multiplier, name='upsample-block')\n",
    "\n",
    "        # TODO Concatenate the upsampled and large input layers using layers.concatenate\n",
    "        layer = layers.concatenate([upsampled_layer, large_ip_layer]\n",
    "                                   #, name='concatenate-1'\n",
    "                                  )\n",
    "        print (\"Concatenating: {} + {} = {}\".format(upsampled_layer.get_shape(), large_ip_layer.get_shape(), layer.get_shape()))\n",
    "\n",
    "        # TODO Add some number of separable convolution layers\n",
    "        layer = separable_conv2d_batchnorm(layer, filters, kernel_size=kernel_size, strides=1\n",
    "                                           #, name=\"separable-convolution-block-1\"\n",
    "                                          )\n",
    "        layer = separable_conv2d_batchnorm(layer, filters, kernel_size=kernel_size, strides=1\n",
    "                                           #, name=\"separable-convolution-block-2\"\n",
    "                                          )\n",
    "\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now that you have the encoder and decoder blocks ready, go ahead and build your FCN architecture! \n",
    "\n",
    "There are three steps:\n",
    "- Add encoder blocks to build the encoder layers. This is similar to how you added regular convolutional layers in your CNN lab.\n",
    "- Add a 1x1 Convolution layer using the conv2d_batchnorm() function. Remember that 1x1 Convolutions require a kernel and stride of 1.\n",
    "- Add decoder blocks for the decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_model(inputs, num_classes):\n",
    "    \n",
    "    print(\"Input: {}\".format(inputs.get_shape()))\n",
    "    \n",
    "    # TODO Add Encoder Blocks. \n",
    "    # Remember that with each encoder layer, the depth of your model (the number of filters) increases.\n",
    "    with tf.name_scope(\"model\"):\n",
    "        layer = conv_1 = encoder_block(inputs, filters=8, kernel_size=3, strides=2, name='encoder-block-1')\n",
    "        print(\"Conv1: {}\".format(layer.get_shape()))\n",
    "        layer = conv_2 = encoder_block(layer, filters=16, kernel_size=3, strides=2, name='encoder-block-2')\n",
    "        print(\"Conv2: {}\".format(layer.get_shape()))\n",
    "        layer = conv_3 = encoder_block(layer, filters=32, kernel_size=3, strides=2, name='encoder-block-3')\n",
    "        print(\"Conv3: {}\".format(layer.get_shape()))\n",
    "        layer = conv_4 = encoder_block(layer, filters=64, kernel_size=3, strides=2, name='encoder-block-4')\n",
    "        print(\"Conv4: {}\".format(layer.get_shape()))\n",
    "#         layer = conv_5 = encoder_block(layer, filters=128, strides=2, name='encoder-block-5')\n",
    "#         print(\"Conv5: {}\".format(layer.get_shape()))\n",
    "\n",
    "    # TODO Add 1x1 Convolution layer using conv2d_batchnorm().\n",
    "        layer = conv_6 = conv2d_batchnorm(layer, filters=256, kernel_size=1, strides=1, name='1x1-convolution')\n",
    "        print(\"1x1: {}\".format(layer.get_shape()))\n",
    "    \n",
    "    # TODO: Add the same number of Decoder Blocks as the number of Encoder Blocks\n",
    "#         layer = conv_7 = decoder_block(layer, conv_4, filters=64, multiplier=2, kernel_size=3, name='decoder-block-5')\n",
    "#         print(\"Conv7: {}\".format(layer.get_shape()))\n",
    "        layer = conv_8 = decoder_block(layer, conv_3, filters=32, multiplier=2, kernel_size=3, name='decoder-block-4')\n",
    "        print(\"Conv8: {}\".format(layer.get_shape()))\n",
    "        layer = conv_9 = decoder_block(layer, conv_2, filters=16, multiplier=2, kernel_size=3, name='decoder-block-3')\n",
    "        print(\"Conv9: {}\".format(layer.get_shape()))\n",
    "        layer = conv_10 = decoder_block(layer, conv_1, filters=8, multiplier=2, kernel_size=3, name='decoder-block-2')\n",
    "        print(\"Conv10: {}\".format(layer.get_shape()))\n",
    "        layer = conv_11 = decoder_block(layer, inputs, filters=3, multiplier=2, kernel_size=3, name='decoder-block-1')\n",
    "        print(\"Conv11: {}\".format(layer.get_shape()))\n",
    "    \n",
    "    # The function returns the output layer of your model. \"x\" is the final layer obtained from the last decoder_block()\n",
    "        layer = output_layer = layers.Conv2D(filters=num_classes, kernel_size=3, strides=1, activation='softmax', padding='same', name='output-layer')(layer)\n",
    "#         tf.summary.histogram('weights', output_layer.get_weights()[0])\n",
    "#         tf.summary.histogram('biases', output_layer.get_weights()[1])\n",
    "#         tf.summary.histogram('biases', output_layer.output)\n",
    "        print (\"Output: {}\".format(layer.get_shape()))\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "This section defines two data augmentation methods:\n",
    "- do_filter(): To filter out image+mask tuples that do not satisfy a criteria. A 'True' return value drops the item from consideration. 'False' retains the tuple.\n",
    "- do_preprocess(): To perform any modification to the image and/or mask, such as vertical flip, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter out any image files that do not satisfy certain criteria (DURING TRAINING ONLY)\n",
    "def do_filter(image_path, mask_path):\n",
    "    contains_hero = misc.imread(mask_path)[:,:,0].max() == 255\n",
    "    if not contains_hero:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > 0.30:\n",
    "            return False # We process only 70% of images without the hero\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # If hero exists, we always accept by returning False\n",
    "    return False\n",
    "\n",
    "# To modify the image and mask before they are used for training (DURING TRAINING ONLY)\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "def do_preprocess(image, mask):\n",
    "    r = random.uniform(0, 1)\n",
    "    if r > 0.50:\n",
    "#        print (\"x\", end=\"\")\n",
    "        image = cv2.flip(image, 0)\n",
    "        mask = cv2.flip(mask, 0)\n",
    "#    else:\n",
    "#        print (\".\", end=\"\")\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Define and tune your hyperparameters.\n",
    "- **batch_size**: number of training samples/images that get propagated through the network in a single pass.\n",
    "- **num_epochs**: number of times the entire training dataset gets propagated through the network.\n",
    "- **batches_per_epoch**: number of batches of training images that go through the network in 1 epoch. We have provided you with a default value. One recommended value to try would be based on the total number of images in training dataset divided by the batch_size.\n",
    "- **batches_per_validation**: number of batches of validation images that go through the network in 1 epoch. This is similar to steps_per_epoch, except batches_per_validation is for the validation dataset. We have provided you with a default value for this as well.\n",
    "- **workers**: maximum number of processes to spin up. This can affect your training speed and is dependent on your hardware. We have provided a recommended value to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is tracked at each training invocation\n",
    "# Any data point that affects the training outcome should go here\n",
    "class T (object):\n",
    "    # Static\n",
    "    image_hw = 160\n",
    "    image_shape = (image_hw, image_hw, 3)\n",
    "    num_classes = 3\n",
    "    \n",
    "    # Changing\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 32 # 200,\n",
    "    num_epochs = 25\n",
    "    batches_per_epoch = 130 #200\n",
    "    batches_per_validation = 25 # 50\n",
    "    workers = 1\n",
    "    train_set_names = ['set0']\n",
    "    validation_set_names = ['set0']\n",
    "    weights_file_name = 'weights.hd5'\n",
    "    arch_file_name = 'architecture.json'\n",
    "    reload_weights = True\n",
    "    skip_training = False\n",
    "    dry_run = False\n",
    "\n",
    "#class Trackables(object):\n",
    "#  def __init__(self, adict):\n",
    "#    self.__dict__.update(adict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training <a id='training'></a>\n",
    "The following cells will use the FCN you created and define an ouput layer based on the size of the processed image and the number of classes recognized. You will define the hyperparameters to compile and train your model.\n",
    "\n",
    "Please Note: For this project, the helper code in `data_iterator.py` will resize the copter images to 160x160x3 to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (?, 160, 160, 3)\n",
      "Conv1: (?, 80, 80, 8)\n",
      "Conv2: (?, 40, 40, 16)\n",
      "Conv3: (?, 20, 20, 32)\n",
      "Conv4: (?, 10, 10, 64)\n",
      "1x1: (?, 10, 10, 256)\n",
      "Concatenating: (?, 20, 20, 256) + (?, 20, 20, 32) = (?, 20, 20, 288)\n",
      "Conv8: (?, 20, 20, 32)\n",
      "Concatenating: (?, 40, 40, 32) + (?, 40, 40, 16) = (?, 40, 40, 48)\n",
      "Conv9: (?, 40, 40, 16)\n",
      "Concatenating: (?, 80, 80, 16) + (?, 80, 80, 8) = (?, 80, 80, 24)\n",
      "Conv10: (?, 80, 80, 8)\n",
      "Concatenating: (?, 160, 160, 8) + (?, 160, 160, 3) = (?, 160, 160, 11)\n",
      "Conv11: (?, 160, 160, 3)\n",
      "Output: (?, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Defining the input and output layers:\n",
    "inputs = layers.Input(T.image_shape)\n",
    "output_layer = fcn_model(inputs, T.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing training from existing model file: ../data/weights/architecture.json\n",
      "Training data: ['../data/masks/train/set0']\n",
      "Initializing the batch iterator...\n",
      "Read 4131 image files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safdar/anaconda3/envs/RoboND/lib/python3.5/site-packages/ipykernel_launcher.py:3: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the batch iterator...\n",
      "Read 1184 image files.\n",
      "Epoch 1/25\n",
      "129/130 [============================>.] - ETA: 2s - loss: 0.2719Epoch 00000: saving model to ../data/weights/history/17_01_2018_11_18_28/00-weights-val_loss(0.1295).h5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12dc857b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 0.12952, saving model to ../data/weights/weights.hd5\n",
      "130/130 [==============================] - 337s - loss: 0.2712 - val_loss: 0.1295\n",
      "Epoch 2/25\n",
      "129/130 [============================>.] - ETA: 3s - loss: 0.1031Epoch 00001: saving model to ../data/weights/history/17_01_2018_11_18_28/01-weights-val_loss(0.1076).h5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4VHX2x/H3SScJJUAISOi9hCKRIhJ0pVtABUVZxIooCoLrwvpzXdfVXXbXBXFFEbssiogoqCBNJVQhIL2GHlpCJ/SQ8/tjLjrEECYkk0ky5/U882TmtpwbNJ/ce7/3XFFVjDHGmKsV4OsCjDHGFG0WJMYYY/LEgsQYY0yeWJAYY4zJEwsSY4wxeWJBYowxJk8sSIzJQkTGisif83tZY4orsftITHEiIjuAR1R1jq9rMcZf2BGJ8SsiEuTrGgqSv+2v8Q0LElNsiMh4oCrwtYiki8gfRaS6iKiIPCwiu4DvnWU/F5H9InJMRBJFpJHbdj4UkZed9zeKSIqIPCMiqSKyT0QevMply4nI1yJyXESWicjLIrIgh/25QUQWichREdktIg84038UkUfclnvAfTvO/g4UkS3AFhF5S0RezbLtqSIy1Hl/jYh8ISJpIrJdRAa5LddSRJKcmg+IyMhc/rMYP2BBYooNVe0L7AJuU9VIVf2X2+z2QAOgs/N5BlAHqACsACbksOmKQGmgMvAwMEZEoq5i2THASWeZfs4rWyJSzanxv0A00AxYmUONWfUAWgENgU+Be0REnG1HAZ2AiSISAHwNrHJqvhl4WkQu/pxGA6NVtRRQC5iUixqMn7AgMf7iRVU9qaqnAVT1fVU9oapngReBpiJS+jLrngdeUtXzqjodSAfq5WZZEQkE7gL+oqqnVHU98FEO9d4HzFHVT51tHVLV3ATJP1T1sLO/8wEF2jnzegKLVXUvcB0Qraovqeo5Vd0GvAP0dtuf2iJSXlXTVXVJLmowfsKCxPiL3RffiEigiIwQka0ichzY4cwqf5l1D6lqhtvnU0BkLpeNBoLc68jyPqsqwNYc5l/JL9tW14iaicC9zqT7+PUIrBpwjXP67KiIHAWeA2Kc+Q8DdYGNzum4W/NQkymmLEhMcXO5YYju0+8DugMdcJ2Gqu5MF++VRRqQAcS6TauSw/K7cZ1Kys5JINztc8Vslsn6c/gU6OmcMmsFfOH2fbarahm3V0lV7QagqltU9V5cpwD/CUwWkYgc6jZ+yILEFDcHgJpXWKYkcBY4hOsX8t+9XZSqXgCmAC+KSLiI1Afuz2GVCUAHEblbRIKcC/XNnHkrgTud7dTGddRwpe//M3AQeBeYqapHnVlLgRMiMkxESjhHa41F5DoAEfm9iESraiZwcZ3MXO6+KeYsSExx8w/geec0zR8us8zHwE5gD7AeKKjz/k/iOgLaD4zHdZRwNrsFVXUX0A14BjiMKzyaOrNHAedwheZH5DxQwN0nuI7CPnH7PheAW3FdzN/Or2Fz8XpRF2CdiKTjuvDe++J1JmMushsSjfEREfknUFFVLzt6y5iiwI5IjCkgIlJfRJqIS0tcp6S+9HVdxuSV3fVqTMEpiet01jW4Tkv9B5jq04qMyQd2assYY0ye2KktY4wxeeIXp7bKly+v1atX93UZxhhTpCxfvvygqkZfaTm/CJLq1auTlJTk6zKMMaZIEZGdnixnp7aMMcbkiVeDRES6iMgmEUkWkeHZzO8jIqtFZI3TLrupM72eiKx0ex0XkaedeS+KyB63ed28uQ/GGGNy5rVTW0630zFARyAFWCYi05yupxdtB9qr6hER6QqMA1qp6iZcd9pe3M4eLh1vP0pVL3m+gjHGGN/w5jWSlkCy05YaEZmIq1HeL0Giqovcll/CpQ3tLroZ2KqqHp2r89T58+dJSUnhzJkz+blZvxQWFkZsbCzBwcG+LsUY4wPeDJLKXNomOwVX19HLeRjXg3yy6o3rJi53T4nI/UAS8IyqHsm6koj0B/oDVK1a9TcbTUlJoWTJklSvXh3neT/mKqgqhw4dIiUlhRo1avi6HGOMDxSKi+0ichOuIBmWZXoIcDvwudvkt3B1d20G7MN1d/BvqOo4VY1X1fjo6N+OXjtz5gzlypWzEMkjEaFcuXJ2ZGeMH/NmkOzh0uctxDrTLiEiTXB1G+2uqoeyzO4KrFDVAxcnqOoBVb3gtLV+B9cptKtiIZI/7OdojH/zZpAsA+qISA3nyKI3MM19ARGpiusZDX1VdXM227iXLKe1RKSS28c7gLX5WrWb9LMZpJ04i7WRMcaYy/PaNRJVzRCRJ4GZQCDwvqquE5EBzvyxwAtAOeBN56/aDFWNB3CewtYReCzLpv/lPOBHcT0iNev8q1KxIhw4kHVqEFf7I4qJgf3781qVMcYUfl69RqKq01W1rqrWUtVXnGljnRBBVR9R1ShVbea84t3WPamq5VT1WJZt9lXVOFVtoqq3q+q+/Kj1tyHi3e0dPXqUN998M9fb7datG0ePHr3yglk88MADTJ48OdfrGWPMlRSKi+3+6HJBkpGRkeN606dPp0yZMt4qyxhjcs2CxEeGDx/O1q1badasGddddx3t2rXj9ttvp2HDhgD06NGDFi1a0KhRI8aNG/fLetWrV+fgwYPs2LGDBg0a8Oijj9KoUSM6derE6dOePQF17ty5NG/enLi4OB566CHOnj37S00NGzakSZMm/OEPrqfUfv755zRu3JimTZuSkJCQzz8FY0yxoKrF/tWiRQvNav369Zd8hvx/5WT79u3aqFEjVVX94YcfNDw8XLdt2/bL/EOHDqmq6qlTp7RRo0Z68OBBVVWtVq2apqWl6fbt2zUwMFB//vlnVVXt1auXjh8//rLfr1+/fvr555/r6dOnNTY2Vjdt2qSqqn379tVRo0bpwYMHtW7dupqZmamqqkeOHFFV1caNG2tKSsol07KT9edpjCn6gCT14HesHZEUEi1btrzkhr7XX3+dpk2b0rp1a3bv3s2WLVt+s06NGjVo1qwZAC1atGDHjh1X/D6bNm2iRo0a1K1bF4B+/fqRmJhI6dKlCQsL4+GHH2bKlCmEh4cD0LZtWx544AHeeecdLly4kA97aowpbixIComIiIhf3v/444/MmTOHxYsXs2rVKpo3b57tDX+hoaG/vA8MDLzi9ZWcBAUFsXTpUnr27Mk333xDly5dABg7diwvv/wyu3fvpkWLFhw6lPVWH2OMv/OL55F4IiYmf0duxcTkPL9kyZKcOHEi23nHjh0jKiqK8PBwNm7cyJIlS/Ktrnr16rFjxw6Sk5OpXbs248ePp3379qSnp3Pq1Cm6detG27ZtqVmzJgBbt26lVatWtGrVihkzZrB7927KlSuXb/UYY4o+CxJHXu/5yFQl7cRZUk+cJVCEa8qEoRp82bu+y5UrR9u2bWncuDElSpQgxi15unTpwtixY2nQoAH16tWjdevWeSvOTVhYGB988AG9evUiIyOD6667jgEDBnD48GG6d+/OmTNnUFVGjhwJwLPPPsuWLVtQVW6++WaaNm2ab7UYY4oHUT+4azs+Pl6zPiFxw4YNNGjQIN+/1+nzF9hz5BSnzl2gVFgwlcuUIDio+J9B9NbP0xjjOyKyXN3u77uc4v8broCVCA6kVnQklUqXIP1sBpsPnODQSWuzYowpvuzUlheICNElQykVFkTK0dPsOXKaY6fOUzmqBKFBgV793gMHDmThwoWXTBs8eDAPPvigV7+vMcZ/WZB4UWhwIDXLR3D41Dn2Hz3DlgPpxJQKo3xkiNc65o4ZM8Yr2zXGmMuxIPEyEaFcRCilQoPZc/Q0+46d5tjp88RGlSAs2LtHJ8YYUxDsGkkBCQ4KoFq5cKqWDedcRiZbUtM5cPwMmXbtxBhTxNkRSQESEcqEhxAZGsTeY2c4cPzML0cn4SH2T2GMKZrst9dFUyrCmXy8IzEsBu7M/uaUoMAAqpYNp0wJ1+muranplIsMpWKpMAIC7GmDxpiixU5tXZSfIeLh9kqVCKZuTCRRESEcTD/L5tQTpJ85n+2ykZGRl93Ojh07aNy48VWXaowxeWFB4mOBAQHERoVTs3wkAmw7eJKUI6e4kJnp69KMMcYjFiQ+Mnz48EuG6r464mUmvTOagX3uoGNCGxo0iuPTSV/kertnzpzhwQcfJC4ujubNm/PDDz8AsG7dOlq2bEmzZs1o0qQJW7Zs4eTJk9xyyy00bdqUxo0b89lnn+Xb/hlj/IddI/GRe+65h6effpqBAwcCMGnSJGbOnMngwYMJCgtndXIKd3f7Hdf/rjPXRIV7vN0xY8YgIqxZs4aNGzfSqVMnNm/ezNixYxk8eDB9+vTh3LlzXLhwgenTp3PNNdfw7bffAq5mkcYYk1sWJD7SvHlzUlNT2bt3L2lpaURFRVGxYkWGDBlCYmIiAQEBpB3Yx7bdezl5ztXQUVWveCPjggULeOqppwCoX78+1apVY/PmzbRp04ZXXnmFlJQU7rzzTurUqUNcXBzPPPMMw4YN49Zbb6Vdu3Ze329jTPFjp7Z8qFevXkyePJnPPvuMe+65hwkTJpCWlsby5ctZuXIlMTExXBMZSEhQIJkKOw+d4lzG1V07ue+++5g2bRolSpSgW7dufP/999StW5cVK1YQFxfH888/z0svvZTPe2iM8QdeDRIR6SIim0QkWUSGZzO/j4isFpE1IrJIRJq6zdvhTF8pIklu08uKyGwR2eJ8jcqXYsOu8AARL2zvnnvuYeLEiUyePJlevXpx7NgxKlSoQHBwMD/88AM7d+4kNDiQWtERiED62Qy2HDjBofTLN4Fs164dEyZMAGDz5s3s2rWLevXqsW3bNmrWrMmgQYPo3r07q1evZu/evYSHh/P73/+eZ599lhUrVuTrj8AY4x+8dmpLRAKBMUBHIAVYJiLTVHW922LbgfaqekREugLjgFZu829S1YNZNj0cmKuqI5xwGg4My3PBl7nnw5saNWrEiRMnqFy5MpUqVaJPnz7cdtttxMXFER8fT/369QHXjYwC1ImJZM+R0+w5epqjp88TW6YEoVnarDzxxBM8/vjjxMXFERQUxIcffkhoaCiTJk1i/PjxBAcHU7FiRZ577jmWLVvGs88+S0BAAMHBwbz11lsF/jMwxhR9XnseiYi0AV5U1c7O5z8BqOo/LrN8FLBWVSs7n3cA8VmDREQ2ATeq6j4RqQT8qKr1cqqlIJ9H4m2qypFT59l37DSqEFMqlPKRoV5rAumpovrzNMZcXmF4HkllYLfb5xRn2uU8DMxw+6zAHBFZLiL93abHqOo+5/1+INtzSCLSX0SSRCQpLS0t99UXUiJC2YgQ6saUJDI0iH3HzpCcms7pcxd8XZoxxk8VilFbInITriC5wW3yDaq6R0QqALNFZKOqJrqvp6oqItkeUqnqOFynyoiPjy8WnRHXrFlD3759L5kWFBzCx1PnkJyaTnTJUCqUCiXAx0cnxhj/4s0g2QNUcfsc60y7hIg0Ad4FuqrqoYvTVXWP8zVVRL4EWgKJwAERqeR2aiv1agv0ZDhtYRIXF8fKlSt/Mz3jQib7jp0h9cSvTSAjQgvubwR7+qMx/s2bp7aWAXVEpIaIhAC9gWnuC4hIVWAK0FdVN7tNjxCRkhffA52Atc7saUA/530/YOrVFBcWFsahQ4eKxS/BoMAAqpQNp3r5CDJV2ZqWzt6jp7mQ6f19U1UOHTpEWFiY17+XMaZw8tqfraqaISJPAjOBQOB9VV0nIgOc+WOBF4BywJvOkUGGc2EnBvjSmRYEfKKq3zmbHgFMEpGHgZ3A3VdTX2xsLCkpKRSn6ycAmaqcPH2eA7susClAKBMe7PUHaIWFhREbG+vV72GMKby8NmqrMMlu1FZxt3T7YYZ9sZrtB09yd3ws/9etIaXDg31dljGmCCkMo7aMD7WsUZYZg9vx+I21+GLFHjqMmsd3awv+XhljTPFnQVKMhQUHMqxLfb56oi3lI0MZ8L/lDJywgrQTZ31dmjGmGLEg8QNxsaWZ9mRbnu1cj9nrD9Bh5Dy+WJ5SLAYaGGN8z4LETwQHBjDwptpMH9yO2hUieebzVTzwwTL2HD3t69KMMUWcBYmfqV0hks8fa8OLtzVk2Y7DdBo5j48X7yCzAIYKG2OKJwsSPxQQIDzQtgYzn07g2mpRvDB1HfeMW8zWtHRfl2aMKYIsSPxYlbLhfPxQS/7dswmb9p+g6+j5vPljMucv2PPijTGesyDxcyJCr/gqzHmmPb+rV4F/fbeJHmMWsnaPPXbXGOMZCxIDQIWSYYzt24K3+lzLgeNn6T5mIf+euZEz562rsDEmZxYk5hJd4yoxZ2gCdzSvzJgfttLt9fkk7Tjs67KMMYWYBYn5jTLhIbzaqykfP9SSs+cz6fX2Yl6cto6TZzN8XZoxphCyIDGXlVA3mllDEujXpjofLd5Bp1GJzNtcvJpcGmPyzoLE5CgiNIgXb2/E54+1ITQ4gH7vL+WZSas4euqcr0szxhQSFiTGI/HVyzJ9UDuevKk2X63cQ4eRicxYs+/KKxpjij0LEuOxsOBA/tC5HtOebEtMqVAen7CCAeOXk3r8jK9LM8b4kAWJybVG15Rm6sC2DOtSn+83pdJh5Dw+T9ptTSCN8VMWJOaqBAUG8PiNtZgxuB31Kpbk2cmruf/9pew+fMrXpRljCpgFicmTWtGRfNa/DX/r3ogVO4/Q+bVEPli4vUCeF2+MKRwsSEyeBQQIfdtUZ+aQBK6rXpa/fr2eu99eTHLqCV+XZowpABYkJt/ERoXz4YPXMfLupmxNS6fb6AW88f0WawJpTDHn1SARkS4isklEkkVkeDbz+4jIahFZIyKLRKSpM72KiPwgIutFZJ2IDHZb50UR2SMiK51XN2/ug8kdEeHOa2OZPaQ9HRvF8Oqszdz+hjWBNKY481qQiEggMAboCjQE7hWRhlkW2w60V9U44G/AOGd6BvCMqjYEWgMDs6w7SlWbOa/p3toHc/WiS4Yy5r5rebtvCw6mu5pAjphhTSCNKY68eUTSEkhW1W2qeg6YCHR3X0BVF6nqEefjEiDWmb5PVVc4708AG4DKXqzVeEnnRhWZM6Q9Pa+NZey8rXQbPZ+l260JpDHFiTeDpDKw2+1zCjmHwcPAjKwTRaQ60Bz4yW3yU84psfdFJCq7jYlIfxFJEpGktDTrD+VLpcOD+WfPJvzv4Vacu5DJ3W8v5s9freXEmfO+Ls0Ykw8KxcV2EbkJV5AMyzI9EvgCeFpVjzuT3wJqAs2AfcB/stumqo5T1XhVjY+OjvZa7cZzN9Qpz6whCTzUtgb/+2knnUcl8sOmVF+XZYzJI28GyR6gitvnWGfaJUSkCfAu0F1VD7lND8YVIhNUdcrF6ap6QFUvqGom8A6uU2imiAgPCeKF2xoyecD1RIQG8eAHyxj62UqOnLQmkMYUVd4MkmVAHRGpISIhQG9gmvsCIlIVmAL0VdXNbtMFeA/YoKojs6xTye3jHcBaL9VvvKhFtSi+GXQDg35Xm2mr9tJh5Dy+Wb3X2qwYUwR5LUhUNQN4EpiJ62L5JFVdJyIDRGSAs9gLQDngTWcob5IzvS3QF/hdNsN8/+UMF14N3AQM8dY+GO8KDQpkaKd6fP3UDVxTpgRPfvIz/ccv54A1gTSmSBF/+AswPj5ek5KSrryg8ZmMC5m8t2A7I2dvJiQogOdvacDd8VVwHZwaY3xBRJaravyVlisUF9uNCQoM4LH2tfju6QQaVCrFsC/W0Ofdn9h1yJpAGlPYWZCYQqVG+QgmPtqaV+5ozOqUY3R+LZH3FlgTSGMKMwsSU+gEBAh9WlVj9tAE2tQqx9++Wc9dby1i8wFrAmlMYWRBYgqtSqVL8F6/eEb3bsbOQye55fX5vD53C+cyrAmkMYWJBYkp1ESE7s0qM2doe7o0rsTI2Zu5/Y0FrNp91NelGWMcFiSmSCgXGcp/723OO/fHc+TUOe54cyF/n76B0+esCaQxvmZBYoqUjg1jmD20PfdcV4VxidvoOjqRxVsPXXlFY4zXWJCYIqdUWDD/uLMJnzzSikyFe99ZwnNfruG4NYE0xicsSEyRdX3t8sx8OoFH29Vg4tJddBqZyPcbD/i6LGP8jgWJKdJKhATyf7c0ZMoTbSldIpiHPkxi8MSfOZR+1telGeM3LEhMsdCsShm+fuoGnu5Qh+lr9tFxVCJTV+6xJpDGFAALElNshAQF8HSHunzzVDuqlA1n8MSVPPJREvuOnfZ1acYUaxYkptipV7EkUx6/nudvacDCrQfpNDKRT37aRaa1WTHGKyxITLEUGCA80q4mM59OoHHl0jz35Rrue3cJOw6e9HVpxhQ7FiSmWKtWLoJPHm3FiDvjWLfnOF1GJ/JO4jZrAmlMPrIgMcWeiNC7ZVVmD23PDbXL88r0Ddz55kI27bcmkMbkBwsS4zcqlg7jnfvj+e+9zUk5cppb/zufUbM3czbD2qwYkxcWJMaviAi3Nb2G2UPbc0tcJUbP3cJt/13Az7uO+Lo0Y4osCxLjl8pGhPBa7+a8/0A8J85kcOdbi/jbN+s5dS7D16UZU+RYkBi/9rv6McwakkCfVlV5b8F2urw2n0XJB31dljFFileDRES6iMgmEUkWkeHZzO8jIqtFZI2ILBKRpldaV0TKishsEdnifI3y5j6Y4q9kWDAv94hjYv/WBAjc9+5PDP9iNcdOWxNIYzzhtSARkUBgDNAVaAjcKyINsyy2HWivqnHA34BxHqw7HJirqnWAuc5nY/Ksdc1yfPd0Ao+1r8mkpN10GjWP2eutCaQxV+LNI5KWQLKqblPVc8BEoLv7Aqq6SFUvXuVcAsR6sG534CPn/UdADy/ug/EzYcGB/KlrA74a2Jao8BAe/TiJJz9ZwUFrAmnMZXkzSCoDu90+pzjTLudhYIYH68ao6j7n/X4gJruNiUh/EUkSkaS0tLTc1m78XJPYMkx78gae6ViXWesO0GHkPL78OcWaQBqTjUJxsV1EbsIVJMNys566/q/O9v9sVR2nqvGqGh8dHZ0PVRp/ExIUwFM31+HbQTdQo3wEQz5bxUMfLmPvUWsCaYw7bwbJHqCK2+dYZ9olRKQJ8C7QXVUPebDuARGp5KxbCUjN57qNuUSdmJJMHnA9L9zakCXbDtNpVCLjl+y0JpDGOLwZJMuAOiJSQ0RCgN7ANPcFRKQqMAXoq6qbPVx3GtDPed8PmOrFfTAGcDWBfOiGGswakkCzKmX481dr6f3OErZbE0hjvBckqpoBPAnMBDYAk1R1nYgMEJEBzmIvAOWAN0VkpYgk5bSus84IoKOIbAE6OJ+NKRBVyoYz/uGW/OuuJmzYd5wuryUydt5WMi5k+ro0Y3xG/OHiYXx8vCYlJfm6DFPMHDh+hj9/tZZZ6w/QuHIp/nVXUxpeU8rXZRmTb0RkuarGX2m5QnGx3ZiiKKZUGG/3bcGbfa5l/7Ez3P7GAv4za5M1gTR+x4LEmDwQEbrFVWL2kPbc3uwa/vt9Mre8voDlO60JpPEfFiTG5IOoiBBG3t2MDx+8jtPnLtBz7CL++vU6Tp61JpCm+PMoSERksIiUEpf3RGSFiHTydnHGFDU31qvAzCEJ9G1djQ8W7qDza4nM32I3xJrizdMjkodU9TjQCYgC+mKjpYzJVmRoEC91b8ykx9oQEhhA3/eW8sfJqzh2yppAmuLJ0yAR52s3YLwzFFdyWN4Yv9eyRlmmD27H4zfW4osVe+gwah7frd3v67KMyXeeBslyEZmFK0hmikhJwAbOG3MFYcGBDOtSn6kD2xIdGcqA/y3niQnLST1xxtelGZNvPLqPREQCgGbANlU9KiJlgVhVXe3tAvOD3UdiCoPzFzIZl7iN0XO3UCI4kBdubcid11ZGxA7uTeGU3/eRtAE2OSHye+B54FheCjTG3wQHBjDwptpMH9SO2hUieebzVfT7YBkpR075ujRj8sTTIHkLOOU8wfAZYCvwsdeqMqYYq10hks8fa8Nfb29E0o7DdB6VyMeLd1gTSFNkeRokGU7L9u7AG6o6BijpvbKMKd4CAoR+11dn5tMJXFstihemruPutxezNS3d16UZk2ueBskJEfkTrmG/3zrXTIK9V5Yx/qFK2XA+fqglr/ZqypbUdLqOns+YH5I5b00gTRHiaZDcA5zFdT/JflzPB/m316oyxo+ICD1bxDJ7aAIdGlTg3zM30WPMQtbuscuQpmjwKEic8JgAlBaRW4EzqmrXSIzJRxVKhvFmnxaM/f21HDh+lu5jFvKv7zZy5rw1gTSFm6ctUu4GlgK9gLuBn0SkpzcLM8ZfdWlciblD23Nn88q8+eNWur0+n6Qdh31dljGX5el9JKuAjqqa6nyOBuaoalMv15cv7D4SU1Qlbk7jT1PWsPfYae5vXY1nu9QnMjTI12UZP5Hf95EEXAwRx6FcrGuMuUoJdaOZNSSBfm2q8/GSnXQelci8zdYE0hQunobBdyIyU0QeEJEHgG+B6d4ryxhzUURoEC/e3ojJA9oQFhxAv/eXMnTSSo6eOufr0owBcvGoXRG5C2jrfJyvql96rap8Zqe2THFx5vwF3vg+mbHztlImPJiXujemW1wlX5dliilPT23ZM9uNKYLW7T3GsC9Ws3bPcbo0qshL3RtRoVSYr8syxUy+XCMRkRMicjyb1wkROZ5/5RpjcqPRNaX56om2DOtSn+83pdJh5DwmJe3GH/4wNIVPjkGiqiVVtVQ2r5KqWupKGxeRLiKySUSSRWR4NvPri8hiETkrIn9wm15PRFa6vY6LyNPOvBdFZI/bvG5Xs+PGFHVBgQE8fmMtvhvcjvoVS/HHyau5//2l7D5sTSBNwfLaqS0RCQQ2Ax2BFGAZcK+qrndbpgJQDegBHFHVVy+znT1AK1XdKSIvAunZLXs5dmrLFHeZmcqEpbsYMX0DmQp/7FKP+9tUJzDAWtSbq5ffw3+vRksgWVW3qeo5YCKupo+/UNVUVV0G5PQM0puBraq603ulGlO0BQQIfVtXY9bQ9rSqWZa/fr2eXmMXkZx6wtelGT/gzSCpDOx2+5ziTMut3sCnWaY9JSKrReR9EYnKbiUR6S8iSSKSlJZm4+6Nf6hcpgQfPHAdo+5pyrZEi+PDAAAW8UlEQVSDJ+k2egFvfL/FmkAaryrUNxWKSAhwO/C52+S3gJq4nti4D/hPduuq6jhVjVfV+OjoaK/XakxhISLc0TyWOUPb07FRDK/O2sxt/13AmhRrAmm8w5tBsgeo4vY51pmWG12BFap64OIEVT2gqhdUNRN4B9cpNGNMFuUjQxlz37W83bcFh0+eo8ebCxkxw5pAmvznzSBZBtQRkRrOkUVvYFout3EvWU5riYj73Vd3AGvzVKUxxVznRhWZPbQ9Pa+NZey8rXQdPZ+fth3ydVmmGPHqDYnO0NzXgEDgfVV9RUQGAKjqWBGpCCQBpYBMIB1oqKrHRSQC2AXUVNVjbtscj+u0lgI7gMdUdV9OddioLWNcFiYfZPiU1ew+fJrft67KsC71KRlmz6gz2bM7291YkBjzq1PnMvjPrM28v3A7lUqF8codcdxUv4KvyzKFUGEY/muMKYTCQ4L4860N+eLx64kIDeLBD5cx5LOVHD5pTSDN1bEgMcZPXVs1im8G3cCgm+vw9aq9dBw5j29W77U2KybXLEiM8WOhQYEM7ViXr5+6gcpRJXjyk5/pP345B46f8XVppgixIDHG0KBSKaY8fj3PdatP4uY0Ooycx8Slu+zoxHjEgsQYA7iaQPZPqMXMpxNoWKkUw6esoc+7P7HrkDWBNDmzIDHGXKJ6+Qg+fbQ1f78jjtUpx+j02jzenb+NC5l2dGKyZ0FijPmNgADhvlZVmT00getrleflbzdw11uL2HzAmkCa37IgMcZcVqXSJXivXzyjezdj1+FT3PL6fEbP2cK5DGsCaX5lQWKMyZGI0L1ZZWYPSaBr40qMmuNqArlq91Ffl2YKCQsSY4xHykWG8vq9zXn3/niOnT7PHW8u5JVv13P6nDWB9HcWJMaYXOnQMIZZQxPo3bIq78zfTpfRiSzeak0g/ZkFiTEm10qFBfP3O+L45NFWANz7zhL+NGUNx8/k9LBTU1xZkBhjrtr1tcrz3eAE+ifU5LNlu+g0MpG5Gw5ceUVTrFiQGGPypERIIM91a8CUJ9pSukQwD3+UxKBPf+ZQ+llfl2YKiAWJMSZfNKtShq+fuoEhHeoyY+0+Oo5KZOrKPdZmxQ9YkBhj8k1IUACDO9Th20HtqFo2nMETV/LIR0nsO3ba16UZL7IgMcbku7oxJfni8et5/pYGLNx6kI4jE5nw004yrc1KsWRBYozxisAA4ZF2NZn1dHuaxJbm/75cy33vLmHHwZO+Ls3kMwsSY4xXVS0XzoRHWjHizjjW7TlO59cSGZe4lYwL1maluLAgMcZ4nYjQu2VVZg9tT7s60fx9+kbuemsRG/cf93VpJh94NUhEpIuIbBKRZBEZns38+iKyWETOisgfsszbISJrRGSliCS5TS8rIrNFZIvzNcqb+2CMyT8VS4fxzv0teOO+5qQcOc2try9g5OzNnM2wNitFmdeCREQCgTFAV6AhcK+INMyy2GFgEPDqZTZzk6o2U9V4t2nDgbmqWgeY63w2xhQRIsKtTa5hztD23Nb0Gl6fu4VbX1/Ail1HfF2auUrePCJpCSSr6jZVPQdMBLq7L6Cqqaq6DMhNX4XuwEfO+4+AHvlRrDGmYEVFhDDqnmZ88MB1pJ/N4K63FvG3b9Zz6lyGr0szueTNIKkM7Hb7nOJM85QCc0RkuYj0d5seo6r7nPf7gZjsVhaR/iKSJCJJaWlpuanbGFOAbqpfgVlDEujTqirvLdhO59cSWZh80NdlmVwozBfbb1DVZrhOjQ0UkYSsC6jrltlsB6ar6jhVjVfV+OjoaC+XaozJi5JhwbzcI47P+rcmKCCAPu/+xPAvVnPstDWBLAq8GSR7gCpun2OdaR5R1T3O11TgS1ynygAOiEglAOdrar5Ua4zxuVY1yzFjcDsea1+TSUm76ThyHrPW7fd1WeYKvBkky4A6IlJDREKA3sA0T1YUkQgRKXnxPdAJWOvMngb0c973A6bma9XGGJ8KCw7kT10b8NXAtpSNCKH/+OUM/GQFaSesCWRhJd5sqCYi3YDXgEDgfVV9RUQGAKjqWBGpCCQBpYBMIB3XCK/yuI5CAIKAT1T1FWeb5YBJQFVgJ3C3qh7OqY74+HhNSkrKaRFjTCF0/kImb8/byutzkwkPDeQvtzWkR7PKiIivS/MLIrI8y6jZ7Jfzh86cFiTGFG3JqSf44+TVrNh1lBvrRfPKHXFULlPC12UVe54GSWG+2G6MMQDUrlCSzwdcz19ua8hP2w7TaeQ8xi+xJpCFhQWJMaZICAwQHmxbg1lDEmheNYo/f7WW3uOWsC0t3del+T0LEmNMkVKlbDjjH27Jv3o2YeP+43QdPZ+x86wJpC9ZkBhjihwR4e74KswZ2p4b60UzYsZGery5kPV7rQmkL1iQGGOKrAqlwni7bzxv9bmW/cfOcvsbC3h15ibOnLcmkAXJgsQYU+R1javEnKEJdG9WmTd+SOaW1+ezfGeOdwWYfGRBYowpFsqEh/Cfu5vy0UMtOXM+k55jF/PitHWcPGtNIL3NgsQYU6y0rxvNzCEJ3N+6Gh8u2kHn1xKZv8Uat3qTBYkxptiJDA3ir90b8/mANoQEBdD3vaU8+/kqjp2yJpDeYEFijCm2rqtelumD2vHEjbWY8vMeOoyax3dr9115RZMrFiTGmGItLDiQP3apz9SBbYmODGXA/1bw+P+Wk3rijK9LKzYsSIwxfqFx5dJMfbItz3aux9yNqXQcmcjk5Sn4Q79Bb7MgMcb4jeDAAAbeVJvpg9pRp0Ikf/h8Ff0+WEbKkVO+Lq1IsyAxxvid2hUimfRYG17q3ojlOw7TaVQiHy3aYU0gr5IFiTHGLwUECPe3qc7MIQnEVy/LX6at4+63F5Ocak0gc8uCxBjj12Kjwvnowev4T6+mbElNp9vo+Yz5IZnz1gTSYxYkxhi/JyLc1SKWOUPb06FhBf49cxPd31jI2j3HfF1akWBPSMzJgtdg03SIiIaI8hBe/tf3Ec778PIQXg4Cg/K/cGOMT3y3dh9/nrqOwyfP0T+hJoNvrkNYcKCvyypwnj4h0X775SQkAoJC4fA22P0TnDoEepnD3RJlfw2XnEInIhpKREGAHQwaU1h1aVyJNjXL8/K363nrx63MXLuff/ZswnXVy/q6tELJjkhyIzMTTh+Bk2lw6qDr68mDzuviNLfppy/TfVQCXEcxnoRORHkIKw0iea/fGJNr87ek8acpa0g5cpr721Tjj13qExnqH3+De3pE4tUgEZEuwGggEHhXVUdkmV8f+AC4Fvg/VX3VmV4F+BiIARQYp6qjnXkvAo8CF7uwPaeq03OqI9+CJLcuZLiOYjwJnZMH4exlzscGBP8aMlcKnYjyEBJpwWNMPjp5NoNXZ23iw0U7uKZ0CV65ozE31qvg67K8zudBIiKBwGagI5ACLAPuVdX1bstUAKoBPYAjbkFSCaikqitEpCSwHOihquudIEm/uKwnfBYkuZVx1hU82YZOlmknD8L5k9lvJyjMCZhyVw6diGgILlGw+2lMEbV85xGGfbGa5NR07ry2Mn++pSFRESG+LstrCsM1kpZAsqpucwqaCHQHfgkSVU0FUkXkFvcVVXUfsM95f0JENgCV3dctloJCodQ1rpcnzp1yO7LJIXRSN7i+Xjib/XaCIy69vpNT6ISXh6Di+z+OMTlpUS2KbwfdwBvfJ/PWj1tJ3JzGS90b07VxRcSPzwJ4M0gqA7vdPqcArXK7ERGpDjQHfnKb/JSI3A8kAc+o6pFs1usP9AeoWrVqbr9t0RASDiFVoYwH+6cK59KvHDrH98C+Va73mZd5IFBo6UvDxUa0GT8SGhTIM53q0bVxJYZ9sZonJqygc6MY/ta9MRVKhfm6PJ/w5qmtnkAXVX3E+dwXaKWqT2az7Itkc7pKRCKBecArqjrFmRYDHMR17eRvuE6BPZRTLUXm1FZhogpnjuUcOu7XeWxEm/FDGRcyeXfBdkbN3kxoUADP39qQXi1ii83RSWE4tbUHqOL2OdaZ5hERCQa+ACZcDBEAVT3gtsw7wDd5L9X8hgiUKON6la995eU9HdGWuuEqR7RFQ0Q5G9FmCpWgwAAGtK9Fp4YxDJ+yhj9OXs20lXv5x51xVCkb7uvyCow3g2QZUEdEauAKkN7AfZ6sKK44fw/YoKojs8yr5FxDAbgDWJt/JZurFhDg/KIv59nyno5o27fKRrSZQq9mdCQTH23NJ0t3MWLGRjqNSuTZzvXod311AgOK/39v3h7+2w14Ddfw3/dV9RURGQCgqmNFpCKu6xylgEwgHWgINAHmA2uc6eAM8xWR8UAzXKe2dgCPuQVLtuzUVjFgI9pMEbH36Gme+3INP25K49qqZfjnXU2oE1PS12VdFZ8P/y1MLEj8kKcj2i5+vdyItpBIz0PHRrQZh6oydeVe/vr1Ok6evcBTv6vNgBtrERxYtK7/WZC4sSAxOfJ0RNvJg79OtxFtxgMH08/y4rR1fLN6H/UrluTfPZsSF1va12V5zILEjQWJyVeejGi7OJLNRrQZYNa6/fx56lrSTpzl0YSaDOlQt0g0gbQgcWNBYnzK6z3abERbUXDs9HlGzNjAp0t3U6N8BP+4M47WNT0cnOIjFiRuLEhMkWI92oq1RckHGT5lDbsOn6JPq6oM71qfkmHBvi4rWxYkbixITLF2yYi2NDh5KIdTbjairTA4dS6DkbM28/7C7cSUCuPvd8RxU/3C1wTSgsSNBYkxbn4Z0eZB6NiINq/6edcR/jh5NVtS0+nR7BpeuK0RZQtRE0gLEjcWJMZcJRvR5nVnMy7w5g9bGfNDMqVLBPPi7Y24tUmlQtFmxYLEjQWJMQUk30a0iWuUmh+NaNu4/zjDJq9mVcoxOjSI4eUejalY2rdNIC1I3FiQGFNIXWlEm3vo+MGItguZyvsLtvOf2ZsIDgjguVsa0Pu6Kj47OrEgcWNBYkwxkdOItqyhU4RHtO04eJLhU1azZNth2tQsx4i74qhWLqLAvv9FFiRuLEiM8VNFeERbZqYycdlu/jF9A+czM/lDp3o82LZGgTaBtCBxY0FijPFIIRzRtu/YaZ7/ci1zN6bStEoZ/nVXE+pVLJgmkBYkbixIjDH57pcRbR6ETh5HtGl4eX7cA3//MZXdZ0sw4KZ6PHFjbUKCvDvAwILEjQWJMcbnVOHM0SuEzpVHtGUiHNUI0gPLULZCZSLLVvTaiLbC8IREY4wxF4kzpLlElIdPHb0Ap4/+JnQCTh7kVMoukrdtI2LvUWod3U85OYFcbkTbvZ9BvS75uy9ZWJAYY0xhFBDI5Z46GguUOnOeETM28slPu6haNpwRfRpwfUX57ZFOxcZeL9VObRljTBG2eOshhk9Zzc5Dp7i3ZVX+1K0+pfKpCaSnp7aK9q2gxhjj59rUKsd3gxPon1CTz5btouPIecxZf6BAa7AgMcaYIq5ESCDPdWvAl0+0JSo8hEc+TmLQpz9zKP0yw5PzmQWJMcYUE02rlGHakzcwtGNdZqzdR4eR81i89ZDXv69Xg0REuojIJhFJFpHh2cyvLyKLReSsiPzBk3VFpKyIzBaRLc7XKG/ugzHGFCUhQQEMurkO3w5qR+PKpalePtzr39NrQSIigcAYoCvQELhXRBpmWewwMAh4NRfrDgfmqmodYK7z2RhjjJu6MSUZ/3ArKpX2/gPIvHlE0hJIVtVtqnoOmAh0d19AVVNVdRlwPhfrdgc+ct5/BPTw1g4YY4y5Mm8GSWVgt9vnFGdaXteNUdV9zvv9QExeijTGGJM3Rfpiu7pugsn2RhgR6S8iSSKSlJaWVsCVGWOM//BmkOwBqrh9jnWm5XXdAyJSCcD5mprdBlR1nKrGq2p8dHR0rgo3xhjjOW8GyTKgjojUEJEQoDcwLR/WnQb0c973A6bmY83GGGNyyWu9tlQ1Q0SeBGYCgcD7qrpORAY488eKSEUgCSgFZIrI00BDVT2e3brOpkcAk0TkYWAncLe39sEYY8yVWa8tY4wx2bJeW8YYYwqEXxyRiEgartNgV6M8cDAfyykKbJ/9g+2zf8jLPldT1SuOVvKLIMkLEUny5NCuOLF99g+2z/6hIPbZTm0ZY4zJEwsSY4wxeWJBcmXjfF2AD9g++wfbZ//g9X22ayTGGGPyxI5IjDHG5IkFiTHGmDyxIHF48DRHEZHXnfmrReRaX9SZnzzY5z7Ovq4RkUUi0tQXdeanK+2z23LXiUiGiPQsyPrymyf7KyI3ishKEVknIvMKusb85sF/16VF5GsRWeXs84O+qDM/icj7IpIqImsvM9+7v79U1e9fuPp5bQVqAiHAKlw9v9yX6QbMAARoDfzk67oLYJ+vB6Kc9139YZ/dlvsemA709HXdXv43LgOsB6o6nyv4uu4C2OfngH8676NxPak1xNe153G/E4BrgbWXme/V3192ROJyxac5Op8/VpclQJmL7eyLKE+eYLlIVY84H5fgaudflHny7wzwFPAFl3lEQRHiyf7eB0xR1V3gemppAdeY3zzZZwVKiogAkbiCJKNgy8xfqpqIaz8ux6u/vyxIXDx5mmNenvhYGOV2fx7G9RdNUXbFfRaRysAdwFsFWJe3ePJvXBeIEpEfRWS5iNxfYNV5hyf7/AbQANgLrAEGq2pmwZTnM179/eW1NvKm+BCRm3AFyQ2+rqUAvAYMU9VM1x+sxV4Q0AK4GSgBLBaRJaq62bdleVVnYCXwO6AWMFtE5qvqcd+WVXRZkLh48jTHvDzxsTDyaH9EpAnwLtBVVQ8VUG3e4sk+xwMTnRApD3QTkQxV/apgSsxXnuxvCnBIVU8CJ0UkEWgKFNUg8WSfHwRGqOviQbKIbAfqA0sLpkSf8OrvLzu15eLJ0xynAfc7ox9aA8dUdV9BF5qPrrjPIlIVmAL0LSZ/oV5xn1W1hqpWV9XqwGTgiSIaIuDZf9dTgRtEJEhEwoFWwIYCrjM/ebLPu3AdgSEiMUA9YFuBVlnwvPr7y45I8OxpjrhG8HQDkoFTuP6qKbI83OcXgHLAm85f6BlahDunerjPxYYn+6uqG0TkO2A1kAm8q6rZDiEtCjz8N/4b8KGIrME1immYqhbp1vIi8ilwI1BeRFKAvwDBUDC/v6xFijHGmDyxU1vGGGPyxILEGGNMnliQGGOMyRMLEmOMMXliQWKMMSZPLEiMKYScjrzf+LoOYzxhQWKMMSZPLEiMyQMR+b2ILHWe5/G2iASKSLqIjHKedTFXRKKdZZuJyBLneRBfikiUM722iMxxno+xQkRqOZuPFJHJIrJRRCY43WoRkREist7Zzqs+2nVjfmFBYsxVEpEGwD1AW1VtBlwA+gARQJKqNgLm4brLGOBjXHdRN8HVdfbi9AnAGFVtiusZMBdbVzQHngYa4nq+RlsRKYerO3EjZzsve3cvjbkyCxJjrt7NuDrnLhORlc7nmrhajXzmLPM/XL2sSgNlVPXiEwg/AhJEpCRQWVW/BFDVM6p6yllmqaqmOC3OVwLVgWPAGeA9EbkTV7sLY3zKgsSYqyfAR6razHnVU9UXs1nuavsQnXV7fwEIUtUMXA9vmgzcCnx3lds2Jt9YkBhz9eYCPUWkAoCIlBWRarj+v7r4rPf7gAWqegw4IiLtnOl9gXmqegJIEZEezjZCnS682RKRSKC0qk4HhuBq+W6MT1n3X2OukqquF5HngVkiEgCcBwYCJ4GWzrxUXNdRAPoBY52g2MavHVj7Am+LyEvONnrl8G1LAlNFJAzXEdHQfN4tY3LNuv8ak89EJF1VI31dhzEFxU5tGWOMyRM7IjHGGJMndkRijDEmTyxIjDHG5IkFiTHGmDyxIDHGGJMnFiTGGGPy5P8B0+fNFYeiLRQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129dcdf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: val_loss improved from 0.12952 to 0.10757, saving model to ../data/weights/weights.hd5\n",
      "130/130 [==============================] - 505s - loss: 0.1031 - val_loss: 0.1076\n",
      "Epoch 3/25\n",
      " 30/130 [=====>........................] - ETA: 292s - loss: 0.0819"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter('../logs')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "# Define the Keras model and compile it for training\n",
    "arch_file_path = os.path.join(\"../data/weights/\", T.arch_file_name)\n",
    "weights_file_path = os.path.join(\"../data/weights/\", T.weights_file_name)\n",
    "model = models.Model(inputs=inputs, outputs=output_layer)\n",
    "model.compile(optimizer=keras.optimizers.Nadam(T.learning_rate), loss='categorical_crossentropy') # lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "if T.reload_weights and os.path.isfile(weights_file_path):\n",
    "    print (\"Continuing training from existing model file:\", arch_file_path)\n",
    "    model.load_weights(weights_file_path)\n",
    "else:\n",
    "    print (\"Building model from a clean slate...\")\n",
    "\n",
    "if not T.skip_training:\n",
    "    ############################\n",
    "    # Data iterators for loading the training and validation data\n",
    "    ############################\n",
    "    train_data_folders = []\n",
    "    for name in T.train_set_names:\n",
    "        train_data_folders.append(os.path.join('..', 'data', 'masks', 'train', name))\n",
    "    print (\"Training data: {}\".format(train_data_folders))\n",
    "    train_iter = data_iterator.BatchIteratorSimple(batch_size=T.batch_size,\n",
    "                                                   data_folders=train_data_folders,\n",
    "                                                   image_shape=T.image_shape,\n",
    "                                                   shift_aug=True,\n",
    "                                                   filter_callback = do_filter,\n",
    "                                                   preprocess_callback = do_preprocess)\n",
    "    validation_data_folders = []\n",
    "    for name in T.validation_set_names:\n",
    "        validation_data_folders.append(os.path.join('..', 'data', 'masks', 'validation', name))\n",
    "    val_iter = data_iterator.BatchIteratorSimple(batch_size=T.batch_size,\n",
    "                                                 data_folders=validation_data_folders,\n",
    "                                                 image_shape=T.image_shape)\n",
    "\n",
    "    # Setting up some callbacks\n",
    "    callbacks = []\n",
    "    callbacks.append(keras.callbacks.TensorBoard(log_dir='../logs'\n",
    "#                                                  , histogram_freq=1, \\\n",
    "#                                                 write_graph=True, \\\n",
    "#                                                 write_grads=True, \\\n",
    "#                                                 batch_size=T.batch_size, \\\n",
    "#                                                 write_images=True)\n",
    "                                                )\n",
    "                    )\n",
    "    callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=1, mode='min'))\n",
    "\n",
    "    ############################\n",
    "    # History:\n",
    "    ############################\n",
    "    history_folder = os.path.join('../data/weights/history', time.strftime('%d_%m_%Y_%H_%M_%S'))\n",
    "    if not os.path.exists(history_folder):\n",
    "        os.makedirs(history_folder)\n",
    "\n",
    "    # Hyperperamaters tracking for history\n",
    "    with open(os.path.join(history_folder, \"T.txt\"), \"w+\") as T_file:\n",
    "        T_file.write(\"{}\".format(vars(T)))\n",
    "\n",
    "    # Weights checkpoints for history\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint(os.path.join(history_folder, '{epoch:02d}-weights-val_loss({val_loss:.4f}).h5'), \n",
    "                                                     monitor='val_loss', \n",
    "                                                     save_weights_only = True,\n",
    "                                                     verbose=1, \n",
    "                                                     period=1))\n",
    "\n",
    "    # Val_loss tracking and model architecture checkpoints for history:\n",
    "    callbacks.append(plotting_tools.LoggerPlotter(model, \n",
    "                                                  arch_file_path=os.path.join(history_folder, T.arch_file_name),\n",
    "                                                  image_path_template=os.path.join(history_folder, \"{:02d}-val-loss-history-plot.png\")))\n",
    "\n",
    "    ############################\n",
    "    # Optimal model tracking:\n",
    "    ############################\n",
    "    if not T.dry_run:\n",
    "        callbacks.append(keras.callbacks.ModelCheckpoint(weights_file_path, \n",
    "                                                         monitor='val_loss', \n",
    "                                                         save_best_only = True,\n",
    "                                                         save_weights_only = True,\n",
    "                                                         mode='auto',\n",
    "                                                         verbose=1, \n",
    "                                                         period=1))\n",
    "\n",
    "        # Also save the architecture\n",
    "        model_arch_json = model.to_json()\n",
    "        with open(arch_file_path, \"w+\") as arch_file:\n",
    "            json.dump(model_arch_json, arch_file)\n",
    "\n",
    "    ############################\n",
    "    # Training:\n",
    "    ############################\n",
    "    model.fit_generator(train_iter,\n",
    "                        steps_per_epoch = T.batches_per_epoch, # the number of batches per epoch,\n",
    "                        epochs = T.num_epochs, # the number of epochs to train for,\n",
    "                        validation_data = val_iter, # validation iterator\n",
    "                        validation_steps = T.batches_per_validation, # the number of batches to validate on\n",
    "                        callbacks=callbacks,\n",
    "                        workers = T.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your trained model weights\n",
    "# model_tools.save_network(model, T.weight_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction <a id='prediction'></a>\n",
    "\n",
    "Now that you have your model trained and saved, you can make predictions on your validation dataset. These predictions can be compared to the mask images, which are the ground truth labels, to evaluate how well your model is doing under different conditions.\n",
    "\n",
    "There are three different predictions available from the helper code provided:\n",
    "- **patrol_with_targ**: Test how well the network can detect the hero from a distance.\n",
    "- **patrol_non_targ**: Test how often the network makes a mistake and identifies the wrong person as the target.\n",
    "- **following_images**: Test how well the network can identify the target while following them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to load a model which you previously trained you can uncomment the codeline that calls the function below.\n",
    "# model = model_tools.load_network(T.weight_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will write predictions to files and return paths to the appropriate directories.\n",
    "The `run_num` parameter is used to define or group all the data for a particular model run. You can change it for different runs. For example, 'run_1', 'run_2' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# files being inferred: 322\n",
      "\t../data/masks/test/patrol_with_targ -> ../data/inferences/patrol_with_targ_run_1\n",
      "# files being inferred: 270\n",
      "\t../data/masks/test/patrol_non_targ -> ../data/inferences/patrol_non_targ_run_1\n",
      "# files being inferred: 542\n",
      "\t../data/masks/test/following_images -> ../data/inferences/following_images_run_1\n"
     ]
    }
   ],
   "source": [
    "run_num = 'run_1'\n",
    "val_with_targ, pred_with_targ = model_tools.write_predictions_grade_set(model,\n",
    "                                        'test', 'patrol_with_targ', run_num) \n",
    "\n",
    "val_no_targ, pred_no_targ = model_tools.write_predictions_grade_set(model, \n",
    "                                        'test', 'patrol_non_targ', run_num) \n",
    "\n",
    "val_following, pred_following = model_tools.write_predictions_grade_set(model,\n",
    "                                        'test', 'following_images', run_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at your predictions, and compare them to the ground truth labels and original images.\n",
    "Run each of the following cells to visualize some sample images from the predictions in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images while following the target\n",
    "im_files = plotting_tools.get_im_file_sample('test', 'following_images', run_num) \n",
    "for i in range(3):\n",
    "    im_tuple = plotting_tools.load_images(im_files[i])\n",
    "    plotting_tools.show_images(im_tuple)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images while at patrol without target\n",
    "im_files = plotting_tools.get_im_file_sample('test', 'patrol_non_targ', run_num) \n",
    "for i in range(3):\n",
    "    im_tuple = plotting_tools.load_images(im_files[i])\n",
    "    plotting_tools.show_images(im_tuple)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# images while at patrol with target\n",
    "im_files = plotting_tools.get_im_file_sample('test', 'patrol_with_targ', run_num) \n",
    "for i in range(3):\n",
    "    im_tuple = plotting_tools.load_images(im_files[i])\n",
    "    plotting_tools.show_images(im_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation <a id='evaluation'></a>\n",
    "Evaluate your model! The following cells include several different scores to help you evaluate your model under the different conditions discussed during the Prediction step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for while the quad is following behind the target. \n",
    "true_pos1, false_pos1, false_neg1, iou1 = scoring_utils.score_run_iou(val_following, pred_following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for images while the quad is on patrol and the target is not visable\n",
    "true_pos2, false_pos2, false_neg2, iou2 = scoring_utils.score_run_iou(val_no_targ, pred_no_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This score measures how well the neural network can detect the target from far away\n",
    "true_pos3, false_pos3, false_neg3, iou3 = scoring_utils.score_run_iou(val_with_targ, pred_with_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all the true positives, etc from the three datasets to get a weight for the score\n",
    "true_pos = true_pos1 + true_pos2 + true_pos3\n",
    "false_pos = false_pos1 + false_pos2 + false_pos3\n",
    "false_neg = false_neg1 + false_neg2 + false_neg3\n",
    "\n",
    "weight = true_pos/(true_pos+false_neg+false_pos)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IoU for the dataset that never includes the hero is excluded from grading\n",
    "final_IoU = (iou1 + iou3)/2\n",
    "print(final_IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the final grade score is \n",
    "final_score = final_IoU * weight\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will plot the graph created:\n",
    "plotting_tools.plot_keras_model(model, \"Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
